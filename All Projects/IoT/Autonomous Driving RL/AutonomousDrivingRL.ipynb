{
 "cells": [
  {
   "attachments": {
    "Reinforcement-learning-process.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "580b23bb",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Python Workshop \n",
    "\n",
    "In this notebook, we aim to explore the concepts/frameworks for developing reinforcement learning models and learn more about their applications in solving IoT related problems.\n",
    "\n",
    "### Firstly, what is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning is a machine learning paradigm that trains an \"agent\" by rewarding desired behavior and punishing undesired behavior. Essentially, the agent's job is to complete a task within its given \"environment\", but rather than directly giving the agent instructions on the best way to do so, the agent interacts with the environment by exploring each possible \"state\". As the agent performs different \"actions\" to reach each state, it will receive \"rewards\" or penalties and learn the patterns of the environment. As it continues to learn, the RL agent will attempt to maximize the total rewards it receives in its environment, in transitioning from the very first state to the very last state.\n",
    "\n",
    "Some key terms to know when working with RL:\n",
    "\n",
    "| Key Terms | Definitions |\n",
    "|:--------:|:--------:|\n",
    "|  Agent   |  The learner/decision maker that interacts with the environment   |\n",
    "|  Environment   |  The external sytem that the RL agent interacts with   |\n",
    "|  State   |  A representation of a scenario/condition in the environment   |\n",
    "|  Action   |  A set of decisions the RL agent can make, given its current state   |\n",
    "|  Reward   |  A numerical value that the environment provides as feedback to the RL agent, based on the action. Positive value implies desired behavior, negative value implies undesired behavior   |\n",
    "\n",
    "A brief diagram showing the RL process:\n",
    "\n",
    "![Reinforcement-learning-process.png](attachment:Reinforcement-learning-process.png)\n",
    "\n",
    "### How can we apply Reinforcement Learning to an IoT application?\n",
    "\n",
    "The Internet of Things (IoT) sector refers to the network of interconnected devices with sensors, processing ability and advanced software technologies that allow data transmission to other devices/servers through various communication protocols. Reinforcement Learning can be used in many ways to automate and optimize these processes. \n",
    "\n",
    "Imagine a scenario where we want to train a self-driving car, rigged with IoT sensors/cameras/GPS, to autonomously drive a target distance down the highway, while minimizing the chance of colliding into other obstacles. How could we create a basic RL model to simulate this scenario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "04dd8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in some basic Python libraries\n",
    "# Make sure to run pip install gym beforehand to get the gym package\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73681390",
   "metadata": {},
   "source": [
    "### Implementing the RL Environment\n",
    "\n",
    "Let's begin by defining an RL environment for the autonomous driving scenario, described above. \n",
    "\n",
    "Using the data generated by the IoT devices on the car, such as the vehicle position, vehicle speed, target position, current lane, obstacle position, etc., the vehicle can respond with actions like lane changing left/right or speeding up/slowing down. These describe the state (observation) and action spaces of the environment. \n",
    "\n",
    "The environment will then evaluate the actions taken by the vehicle and calculate the corresponding reward, based on the current state. Successfully reaching the target position will yield a great positive reward, while being in the same lane as an upcoming obstacle yields a great negative reward. We can add other factors like speed adjustment by implementing a small negative reward for each timestep, which will encourage the vehicle to reach the target position as fast as possible, while staying under the max speed limit. \n",
    "\n",
    "Finally, we need the environment to reset back to its initial state, once the episode ends. In our environment, the episode completes when the vehicle successfully reaches its target position.\n",
    "\n",
    "In summary, we need to implement a class for our RL environment with the following functions:\n",
    "\n",
    "- Initializing States/Actions: Creating Action and Observation/State Spaces\n",
    "\n",
    "- Step Function: Defining how the actions taken by the agent affect the state + calculating reward\n",
    "\n",
    "- Reset Function: Resetting all state observations and returning to initial state\n",
    "\n",
    "By inheriting these default properties from the OpenAI ```gym``` environment, we can easily customize a class to simulate our autonomous driving scenario!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "52d8162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutonomousDrivingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(AutonomousDrivingEnv, self).__init__()\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        # 4 Discrete Actions: Lane Change Left, Lane Change Right, Speed Up, Slow Down\n",
    "        self.action_space = spaces.Discrete(4) \n",
    "        # 6 State Variables/Observations: Vehicle Position, Vehicle Speed, Obstacle Position + Lane, Target Position, Current Lane\n",
    "        self.observation_space = spaces.Box(low=np.array([0.0, 1.0, 0.0, 0.0, 0, 0]), high=np.array([1000.0, 10.0, 1000.0, 1000.0, 2, 2]), dtype=np.float32)\n",
    "        \n",
    "        # Initialize state variables\n",
    "        self.vehicle_position = 0.0\n",
    "        self.vehicle_speed = 1.0\n",
    "        self.obstacle_position = -1 # No obstacle yet\n",
    "        self.target_position = 1000.0\n",
    "        self.current_lane = 1\n",
    "        self.obstacle_lane = -1\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Implement how the environment responds to actions taken by the agent\n",
    "        # Calculate the reward\n",
    "        # Update the state \n",
    "        if action == 1:  # Left lane change\n",
    "            self.current_lane = max(0, self.current_lane - 1)\n",
    "        elif action == 2:  # Right lane change\n",
    "            self.current_lane = min(2, self.current_lane + 1)\n",
    "        elif action == 3:  # Speed up\n",
    "            self.vehicle_speed = min(self.vehicle_speed + 1, 10)\n",
    "        elif action == 4:  # Speed down\n",
    "            self.vehicle_speed = max(self.vehicle_speed - 1, 1)\n",
    "            \n",
    "        # Update vehicle position based on speed\n",
    "        self.vehicle_position += self.vehicle_speed\n",
    "\n",
    "        # Update obstacle position (randomly simulate obstacles)\n",
    "        if np.random.rand() < 0.3:  # 30% chance of an obstacle appearing\n",
    "            self.obstacle_lane = np.random.randint(0, 2)\n",
    "            self.obstacle_position = np.random.randint(0, 1000)\n",
    "\n",
    "        # Used helper function to define reward for clarity\n",
    "        reward = self._calculate_reward()\n",
    "        \n",
    "        # Checking if episode is done\n",
    "        done = (self.vehicle_position >= self.target_position)\n",
    "\n",
    "        # Returning the observation vector and additional info\n",
    "        return self._get_state(), reward, done, {}\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        if self.vehicle_position >= self.target_position:\n",
    "            return 100  # Positive reward for reaching the target\n",
    "        elif self.obstacle_position >= 0 and self.current_lane == self.obstacle_lane and self.vehicle_position <= self.obstacle_position:\n",
    "            return -100  # Negative reward for colliding with an obstacle\n",
    "        else:\n",
    "            return -1  # Small negative reward for each time step\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the environment to an initial state\n",
    "        self.vehicle_position = 0.0\n",
    "        self.vehicle_speed = 1.0\n",
    "        self.obstacle_position = -1\n",
    "        self.target_position = 1000.0\n",
    "        self.current_lane = 1\n",
    "        self.obstacle_lane = -1\n",
    "        # Return state\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # Return the current state/observation vector as dictionary\n",
    "        return {\n",
    "            'vehicle_position': self.vehicle_position,\n",
    "            'vehicle_speed': self.vehicle_speed,\n",
    "            'obstacle_position': self.obstacle_position,\n",
    "            'target_position': self.target_position,\n",
    "            'current_lane': self.current_lane,\n",
    "            'obstacle_lane': self.obstacle_lane\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a1f56e",
   "metadata": {},
   "source": [
    "### Testing our Environment with a Random Agent\n",
    "\n",
    "For testing our custom RL environment, we will implement a very simple Random Agent algorithm like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8559a220",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 1\n",
      "State: {'vehicle_position': 1.0, 'vehicle_speed': 1.0, 'obstacle_position': -1, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': -1}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 2\n",
      "State: {'vehicle_position': 2.0, 'vehicle_speed': 1.0, 'obstacle_position': -1, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': -1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 3\n",
      "State: {'vehicle_position': 3.0, 'vehicle_speed': 1.0, 'obstacle_position': -1, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': -1}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 4\n",
      "State: {'vehicle_position': 4.0, 'vehicle_speed': 1.0, 'obstacle_position': 291, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 5\n",
      "State: {'vehicle_position': 5.0, 'vehicle_speed': 1.0, 'obstacle_position': 291, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 2\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 6\n",
      "State: {'vehicle_position': 6.0, 'vehicle_speed': 1.0, 'obstacle_position': 291, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 7\n",
      "State: {'vehicle_position': 7.0, 'vehicle_speed': 1.0, 'obstacle_position': 147, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 8\n",
      "State: {'vehicle_position': 8.0, 'vehicle_speed': 1.0, 'obstacle_position': 550, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 9\n",
      "State: {'vehicle_position': 9.0, 'vehicle_speed': 1.0, 'obstacle_position': 550, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 10\n",
      "State: {'vehicle_position': 10.0, 'vehicle_speed': 1.0, 'obstacle_position': 263, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 11\n",
      "State: {'vehicle_position': 11.0, 'vehicle_speed': 1.0, 'obstacle_position': 542, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 12\n",
      "State: {'vehicle_position': 13.0, 'vehicle_speed': 2.0, 'obstacle_position': 542, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 13\n",
      "State: {'vehicle_position': 15.0, 'vehicle_speed': 2.0, 'obstacle_position': 464, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 14\n",
      "State: {'vehicle_position': 18.0, 'vehicle_speed': 3.0, 'obstacle_position': 464, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 15\n",
      "State: {'vehicle_position': 21.0, 'vehicle_speed': 3.0, 'obstacle_position': 193, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 16\n",
      "State: {'vehicle_position': 25.0, 'vehicle_speed': 4.0, 'obstacle_position': 642, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 17\n",
      "State: {'vehicle_position': 29.0, 'vehicle_speed': 4.0, 'obstacle_position': 184, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 18\n",
      "State: {'vehicle_position': 33.0, 'vehicle_speed': 4.0, 'obstacle_position': 184, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 19\n",
      "State: {'vehicle_position': 37.0, 'vehicle_speed': 4.0, 'obstacle_position': 184, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 20\n",
      "State: {'vehicle_position': 41.0, 'vehicle_speed': 4.0, 'obstacle_position': 184, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 21\n",
      "State: {'vehicle_position': 46.0, 'vehicle_speed': 5.0, 'obstacle_position': 184, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 22\n",
      "State: {'vehicle_position': 51.0, 'vehicle_speed': 5.0, 'obstacle_position': 184, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 23\n",
      "State: {'vehicle_position': 56.0, 'vehicle_speed': 5.0, 'obstacle_position': 712, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 24\n",
      "State: {'vehicle_position': 62.0, 'vehicle_speed': 6.0, 'obstacle_position': 712, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 25\n",
      "State: {'vehicle_position': 68.0, 'vehicle_speed': 6.0, 'obstacle_position': 712, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 26\n",
      "State: {'vehicle_position': 74.0, 'vehicle_speed': 6.0, 'obstacle_position': 712, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 27\n",
      "State: {'vehicle_position': 80.0, 'vehicle_speed': 6.0, 'obstacle_position': 530, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 28\n",
      "State: {'vehicle_position': 86.0, 'vehicle_speed': 6.0, 'obstacle_position': 530, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 29\n",
      "State: {'vehicle_position': 92.0, 'vehicle_speed': 6.0, 'obstacle_position': 530, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 30\n",
      "State: {'vehicle_position': 98.0, 'vehicle_speed': 6.0, 'obstacle_position': 530, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 31\n",
      "State: {'vehicle_position': 104.0, 'vehicle_speed': 6.0, 'obstacle_position': 530, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 32\n",
      "State: {'vehicle_position': 110.0, 'vehicle_speed': 6.0, 'obstacle_position': 867, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 33\n",
      "State: {'vehicle_position': 117.0, 'vehicle_speed': 7.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 3\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 34\n",
      "State: {'vehicle_position': 124.0, 'vehicle_speed': 7.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 35\n",
      "State: {'vehicle_position': 131.0, 'vehicle_speed': 7.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 36\n",
      "State: {'vehicle_position': 138.0, 'vehicle_speed': 7.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 37\n",
      "State: {'vehicle_position': 145.0, 'vehicle_speed': 7.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 38\n",
      "State: {'vehicle_position': 152.0, 'vehicle_speed': 7.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 39\n",
      "State: {'vehicle_position': 159.0, 'vehicle_speed': 7.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 40\n",
      "State: {'vehicle_position': 166.0, 'vehicle_speed': 7.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 41\n",
      "State: {'vehicle_position': 173.0, 'vehicle_speed': 7.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 42\n",
      "State: {'vehicle_position': 181.0, 'vehicle_speed': 8.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 43\n",
      "State: {'vehicle_position': 189.0, 'vehicle_speed': 8.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 44\n",
      "State: {'vehicle_position': 197.0, 'vehicle_speed': 8.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 45\n",
      "State: {'vehicle_position': 205.0, 'vehicle_speed': 8.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 46\n",
      "State: {'vehicle_position': 213.0, 'vehicle_speed': 8.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 47\n",
      "State: {'vehicle_position': 221.0, 'vehicle_speed': 8.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 48\n",
      "State: {'vehicle_position': 229.0, 'vehicle_speed': 8.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 49\n",
      "State: {'vehicle_position': 237.0, 'vehicle_speed': 8.0, 'obstacle_position': 956, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 50\n",
      "State: {'vehicle_position': 245.0, 'vehicle_speed': 8.0, 'obstacle_position': 775, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 51\n",
      "State: {'vehicle_position': 253.0, 'vehicle_speed': 8.0, 'obstacle_position': 775, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 52\n",
      "State: {'vehicle_position': 261.0, 'vehicle_speed': 8.0, 'obstacle_position': 605, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 53\n",
      "State: {'vehicle_position': 269.0, 'vehicle_speed': 8.0, 'obstacle_position': 605, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 54\n",
      "State: {'vehicle_position': 277.0, 'vehicle_speed': 8.0, 'obstacle_position': 888, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 55\n",
      "State: {'vehicle_position': 286.0, 'vehicle_speed': 9.0, 'obstacle_position': 258, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 56\n",
      "State: {'vehicle_position': 295.0, 'vehicle_speed': 9.0, 'obstacle_position': 258, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 57\n",
      "State: {'vehicle_position': 304.0, 'vehicle_speed': 9.0, 'obstacle_position': 258, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 58\n",
      "State: {'vehicle_position': 313.0, 'vehicle_speed': 9.0, 'obstacle_position': 258, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 59\n",
      "State: {'vehicle_position': 322.0, 'vehicle_speed': 9.0, 'obstacle_position': 258, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 60\n",
      "State: {'vehicle_position': 331.0, 'vehicle_speed': 9.0, 'obstacle_position': 258, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 61\n",
      "State: {'vehicle_position': 340.0, 'vehicle_speed': 9.0, 'obstacle_position': 258, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 62\n",
      "State: {'vehicle_position': 349.0, 'vehicle_speed': 9.0, 'obstacle_position': 258, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 63\n",
      "State: {'vehicle_position': 358.0, 'vehicle_speed': 9.0, 'obstacle_position': 258, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 64\n",
      "State: {'vehicle_position': 367.0, 'vehicle_speed': 9.0, 'obstacle_position': 585, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 65\n",
      "State: {'vehicle_position': 376.0, 'vehicle_speed': 9.0, 'obstacle_position': 585, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 66\n",
      "State: {'vehicle_position': 385.0, 'vehicle_speed': 9.0, 'obstacle_position': 585, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 67\n",
      "State: {'vehicle_position': 395.0, 'vehicle_speed': 10.0, 'obstacle_position': 585, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 68\n",
      "State: {'vehicle_position': 405.0, 'vehicle_speed': 10.0, 'obstacle_position': 544, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 69\n",
      "State: {'vehicle_position': 415.0, 'vehicle_speed': 10.0, 'obstacle_position': 544, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 70\n",
      "State: {'vehicle_position': 425.0, 'vehicle_speed': 10.0, 'obstacle_position': 437, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 71\n",
      "State: {'vehicle_position': 435.0, 'vehicle_speed': 10.0, 'obstacle_position': 437, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 72\n",
      "State: {'vehicle_position': 445.0, 'vehicle_speed': 10.0, 'obstacle_position': 451, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 73\n",
      "State: {'vehicle_position': 455.0, 'vehicle_speed': 10.0, 'obstacle_position': 451, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 74\n",
      "State: {'vehicle_position': 465.0, 'vehicle_speed': 10, 'obstacle_position': 165, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 75\n",
      "State: {'vehicle_position': 475.0, 'vehicle_speed': 10, 'obstacle_position': 756, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 76\n",
      "State: {'vehicle_position': 485.0, 'vehicle_speed': 10, 'obstacle_position': 756, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 77\n",
      "State: {'vehicle_position': 495.0, 'vehicle_speed': 10, 'obstacle_position': 197, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 78\n",
      "State: {'vehicle_position': 505.0, 'vehicle_speed': 10, 'obstacle_position': 197, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 79\n",
      "State: {'vehicle_position': 515.0, 'vehicle_speed': 10, 'obstacle_position': 843, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 80\n",
      "State: {'vehicle_position': 525.0, 'vehicle_speed': 10, 'obstacle_position': 80, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 81\n",
      "State: {'vehicle_position': 535.0, 'vehicle_speed': 10, 'obstacle_position': 80, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 82\n",
      "State: {'vehicle_position': 545.0, 'vehicle_speed': 10, 'obstacle_position': 80, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 83\n",
      "State: {'vehicle_position': 555.0, 'vehicle_speed': 10, 'obstacle_position': 80, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 84\n",
      "State: {'vehicle_position': 565.0, 'vehicle_speed': 10, 'obstacle_position': 185, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 85\n",
      "State: {'vehicle_position': 575.0, 'vehicle_speed': 10, 'obstacle_position': 397, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 86\n",
      "State: {'vehicle_position': 585.0, 'vehicle_speed': 10, 'obstacle_position': 397, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 87\n",
      "State: {'vehicle_position': 595.0, 'vehicle_speed': 10, 'obstacle_position': 397, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 88\n",
      "State: {'vehicle_position': 605.0, 'vehicle_speed': 10, 'obstacle_position': 397, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 89\n",
      "State: {'vehicle_position': 615.0, 'vehicle_speed': 10, 'obstacle_position': 397, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 90\n",
      "State: {'vehicle_position': 625.0, 'vehicle_speed': 10, 'obstacle_position': 397, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 91\n",
      "State: {'vehicle_position': 635.0, 'vehicle_speed': 10, 'obstacle_position': 102, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 92\n",
      "State: {'vehicle_position': 645.0, 'vehicle_speed': 10, 'obstacle_position': 102, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 93\n",
      "State: {'vehicle_position': 655.0, 'vehicle_speed': 10, 'obstacle_position': 102, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 94\n",
      "State: {'vehicle_position': 665.0, 'vehicle_speed': 10, 'obstacle_position': 102, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 95\n",
      "State: {'vehicle_position': 675.0, 'vehicle_speed': 10, 'obstacle_position': 102, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 96\n",
      "State: {'vehicle_position': 685.0, 'vehicle_speed': 10, 'obstacle_position': 102, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 97\n",
      "State: {'vehicle_position': 695.0, 'vehicle_speed': 10, 'obstacle_position': 102, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 98\n",
      "State: {'vehicle_position': 705.0, 'vehicle_speed': 10, 'obstacle_position': 228, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 1}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 99\n",
      "State: {'vehicle_position': 715.0, 'vehicle_speed': 10, 'obstacle_position': 760, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 100\n",
      "State: {'vehicle_position': 725.0, 'vehicle_speed': 10, 'obstacle_position': 760, 'target_position': 1000.0, 'current_lane': 2, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 101\n",
      "State: {'vehicle_position': 735.0, 'vehicle_speed': 10, 'obstacle_position': 760, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 102\n",
      "State: {'vehicle_position': 745.0, 'vehicle_speed': 10, 'obstacle_position': 760, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 103\n",
      "State: {'vehicle_position': 755.0, 'vehicle_speed': 10, 'obstacle_position': 83, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 104\n",
      "State: {'vehicle_position': 765.0, 'vehicle_speed': 10, 'obstacle_position': 200, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 1}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 105\n",
      "State: {'vehicle_position': 775.0, 'vehicle_speed': 10, 'obstacle_position': 200, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 106\n",
      "State: {'vehicle_position': 785.0, 'vehicle_speed': 10, 'obstacle_position': 200, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 107\n",
      "State: {'vehicle_position': 795.0, 'vehicle_speed': 10, 'obstacle_position': 200, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 108\n",
      "State: {'vehicle_position': 805.0, 'vehicle_speed': 10, 'obstacle_position': 582, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 109\n",
      "State: {'vehicle_position': 815.0, 'vehicle_speed': 10, 'obstacle_position': 807, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 110\n",
      "State: {'vehicle_position': 825.0, 'vehicle_speed': 10, 'obstacle_position': 807, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 111\n",
      "State: {'vehicle_position': 835.0, 'vehicle_speed': 10, 'obstacle_position': 807, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 112\n",
      "State: {'vehicle_position': 845.0, 'vehicle_speed': 10, 'obstacle_position': 711, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 113\n",
      "State: {'vehicle_position': 855.0, 'vehicle_speed': 10, 'obstacle_position': 219, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 114\n",
      "State: {'vehicle_position': 865.0, 'vehicle_speed': 10, 'obstacle_position': 219, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 115\n",
      "State: {'vehicle_position': 875.0, 'vehicle_speed': 10, 'obstacle_position': 946, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -100\n",
      "Episode done: False\n",
      "Time step: 116\n",
      "State: {'vehicle_position': 885.0, 'vehicle_speed': 10, 'obstacle_position': 379, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 117\n",
      "State: {'vehicle_position': 895.0, 'vehicle_speed': 10, 'obstacle_position': 379, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 118\n",
      "State: {'vehicle_position': 905.0, 'vehicle_speed': 10, 'obstacle_position': 379, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 119\n",
      "State: {'vehicle_position': 915.0, 'vehicle_speed': 10, 'obstacle_position': 379, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 120\n",
      "State: {'vehicle_position': 925.0, 'vehicle_speed': 10, 'obstacle_position': 379, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 121\n",
      "State: {'vehicle_position': 935.0, 'vehicle_speed': 10, 'obstacle_position': 851, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 122\n",
      "State: {'vehicle_position': 945.0, 'vehicle_speed': 10, 'obstacle_position': 851, 'target_position': 1000.0, 'current_lane': 1, 'obstacle_lane': 0}\n",
      "Action taken: 2\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 123\n",
      "State: {'vehicle_position': 955.0, 'vehicle_speed': 10, 'obstacle_position': 817, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 0}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 124\n",
      "State: {'vehicle_position': 965.0, 'vehicle_speed': 10, 'obstacle_position': 547, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 1\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 125\n",
      "State: {'vehicle_position': 975.0, 'vehicle_speed': 10, 'obstacle_position': 547, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 126\n",
      "State: {'vehicle_position': 985.0, 'vehicle_speed': 10, 'obstacle_position': 547, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 127\n",
      "State: {'vehicle_position': 995.0, 'vehicle_speed': 10, 'obstacle_position': 547, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 3\n",
      "Reward received: -1\n",
      "Episode done: False\n",
      "Time step: 128\n",
      "State: {'vehicle_position': 1005.0, 'vehicle_speed': 10, 'obstacle_position': 547, 'target_position': 1000.0, 'current_lane': 0, 'obstacle_lane': 1}\n",
      "Action taken: 0\n",
      "Reward received: 100\n",
      "Episode done: True\n",
      "Episode done!\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the environment\n",
    "env = AutonomousDrivingEnv()\n",
    "\n",
    "# Define and set up the RandomAgent\n",
    "class RandomAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, _):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "# Initialize the RandomAgent with the environment's action space\n",
    "agent = RandomAgent(env.action_space)\n",
    "\n",
    "# Example usage of the environment\n",
    "obs = env.reset()\n",
    "for timestep in range(1000):  # Limit the number of timesteps for demonstration\n",
    "    action = env.action_space.sample()  # Random action for testing\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    # Print the information for each time step\n",
    "    print(f\"Time step: {timestep + 1}\")\n",
    "    print(f\"State: {obs}\")\n",
    "    print(f\"Action taken: {action}\")\n",
    "    print(f\"Reward received: {reward}\")\n",
    "    print(f\"Episode done: {done}\")\n",
    "    \n",
    "    if done:\n",
    "        print(\"Episode done!\")\n",
    "        break\n",
    "\n",
    "# Close the environment when done\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59994093",
   "metadata": {},
   "source": [
    "For each time step, we can see the state observations, the agent action taken, the rewards received and the status of the episode. With each time step taken, the vehicle is being penalized with a -1 reward. This reward encourages the agent to increase the vehicle speed to reach the target as fast as possible. At the 5th time step, however, we can see the vehicle was penalized heavily with a -100 for being in the same lane as an upcoming obstacle (obstacle lane == vehicle lane && obstacle position > vehicle position). As a response, the agent will move the vehicle into a different lane to avoid the obstacle. This process repeats, until the 128th time step, where it seems our vehicle reaches the target position and is rewarded greatly, completing the episode. The agent is successfully undergoing reinforcement learning in our autonomous driving environment! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
